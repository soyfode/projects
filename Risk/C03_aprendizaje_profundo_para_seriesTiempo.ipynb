{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cdbd1bb-8d30-456c-8bab-c0a1da9267bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Activation,\n",
    "Flatten, MaxPooling2D,SimpleRNN)\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6932780-d273-404c-b1a3-b07a2ab55130",
   "metadata": {},
   "source": [
    "# Aprendizaje profundo para modelado de series de tiempo\n",
    "En Deep Learning destacan dos métodos por su capacidad de incluir períodos de tiempo más largos: “Recurrent Neural Network” (RNN) y “Long Short-Term Memory” (LSTM).\n",
    "\n",
    "## Red neuronal recurrente\n",
    "RNN, tiene al menos una conexión de retroalimentación para que la red pueda aprender secuencias. Se tiene los siguente formas:\n",
    "\n",
    "1. Uno a uno, una salida y una entrada.\n",
    "2. Uno a muchos, una entrada y varias salidas.\n",
    "3. Muchos a uno, múltiples entradas para una salida.\n",
    "4. Muchos a Muchos, múltiples salidas y entradas.\n",
    "\n",
    "RNN tiene entradas tridimensionales:\n",
    "\n",
    "- Tamaño de lote.- Denota el número de observaciones o el número de filas de un dato.\n",
    "- Pasos de tiempo.- Son la cantidad de veces que se alimenta el modelo.\n",
    "- Número de características.- El número 1 característica, es el número de columnas de cada muestra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afdb9233-8c53-4c52-9f7b-5fa7738cd249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el número de pasos para alimentar al modelo RNN. \n",
    "n_steps = 13\n",
    "\n",
    "# Definir el número de característica como 1. \n",
    "n_features = 1\n",
    "\n",
    "# Llamar a un modelo secuencial para ejecutar RNN. \n",
    "model = Sequential()\n",
    "\n",
    "# Identificación del número de neuronas ocultas y función de activación, forma de entrada. \n",
    "model.add(SimpleRNN(512, activation='relu',\n",
    "input_shape=(n_steps, n_features),\n",
    "return_sequences=True))\n",
    "\n",
    "# Poner una capa de abandono para evitar el sobreajuste. \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Agregando una capa oculta más con 256 neuronas con función de activación relu. \n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "\n",
    "# Aplanando el modelo para transformar una matriz tridimensional en un vector. \n",
    "model.add(Flatten())\n",
    "\n",
    "# Agregar una capa de salida con función de activación lineal. \n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilando el modelo RNN.\n",
    "model.compile(optimizer='rmsprop', loss='mean_squared_error',metrics=['mse'])\n",
    "\n",
    "# Escribir una función llamada función split_sequence para definir el período de retrospectiva.\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequence)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(sequence) - 1:\n",
    "            break\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6902834-8f9d-4336-9f5d-2e079a2e3436",
   "metadata": {},
   "source": [
    "### Desventajas\n",
    "- Problema de gradiente que desaparece y explosivo.\n",
    "    La desaparición del gradiente es un problema común en el aprendizaje profundo y no está diseñado adecuadamente. El problema del gradiente de desaparición surge si el gradiente tiende a reducirse a medida que realizamos la retropropagación. Implica que las neuronas aprenden tan lentamente que la optimización se detiene. A diferencia del problema del gradiente que desaparece, el problema del gradiente explosivo se produce cuando pequeños cambios en la retropropagación dan como resultado grandes actualizaciones en los pesos durante el proceso de optimización.\n",
    "- Entrenar un RNN es una tarea difícil ya que requiere una cantidad considerable de datos.\n",
    "- RNN no puede procesar secuencias muy largas cuando se utiliza la función de activación `tanh`.\n",
    "\n",
    "### Funciones de activación\n",
    "Las funciones de activación son ecuaciones matemáticas que se utilizan para determinar la salida en la estructura de una red neuronal. La función de activación es una herramienta para introducir no linealidad en las capas ocultas de modo que podamos modelar los problemas no lineales. De las funciones de activación, las más famosas son las siguientes: \n",
    "\n",
    "#### Sigmoide \n",
    "Esta función de activación nos permite incorporar pequeñas cantidades de salida a medida que introducimos pequeños cambios en el modelo. Toma valores entre 0 y 1. La representación matemática del sigmoide es:\n",
    "$$sigmoid(x) = \\frac{1}{1+e^{-\\sum_i w_ix_i - b}}.$$\n",
    "donde $w$ es el peso, $x$ denota datos, $b$ representa el sesgo y el subíndice $i$ muestra las características. \n",
    "\n",
    "#### Tanh \n",
    "Si manejas números negativos, `tanh` es tu función de activación. A diferencia de la función sigmoidea, oscila entre -1 y 1. La fórmula tanh es:\n",
    "$$\\tanh(x) = \\frac{sinh(x)}{\\cosh(x)}$$\n",
    "\n",
    "#### Lineal\n",
    "El uso de la función de activación lineal nos permite construir una relación lineal entre variables independientes y dependientes. La función de activación lineal toma las entradas y las multiplica por los pesos y forma las salidas. proporcional a la entrada. Por lo tanto, es una función de activación conveniente para modelos de series temporales. La función de activación lineal toma la forma de:\n",
    "$$f(x)=wx.$$\n",
    "\n",
    "#### Lineal rectificado\n",
    "La función de activación lineal rectificada, conocida como `ReLu`, puede tomar 0 si la entrada es cero o está por debajo de cero. Si la entrada es mayor que 0, aumenta en línea con $x$. Matemáticamente: \n",
    "$$ReLu(x) = max(0, x).$$\n",
    "\n",
    "#### Softmax\n",
    "Es ampliamente aplicable a problemas de clasificación como en `sigmoide` porque `softmax` convierte la entrada en una distribución probabilística proporcional al exponencial de los números de entrada:\n",
    "$$softmax(x_i) = \\dfrac{e ^{x_i}}{\\sum_i e^{x_i}}$$\n",
    "\n",
    "## Memoria a largo y corto plazo. LSTM\n",
    "Se basa en la unidad recurrente cerrada (GRU). Se propone GRU para abordar el problema del gradiente de desaparición, que es un problema común en la estructira de la red neuronal y ocurre cuando la actualización del peso se vuelve demasiado pequeña para crear un cambio significativo en la red. GRU consta de dos puertas:\n",
    "\n",
    "- Actualizar.\n",
    "- Restablecer.\n",
    "\n",
    "Cuando se detecta que una observación temprana es muy importante, no actualizamos el estado oculto. De manera similar, cuando las primeras observaciones no son significativas, se restablece el estado. \n",
    "\n",
    "Las RNN tiene la capacidad de conectar información pasada y presente, el problema radica cuando las dependencias a largo plazo entran en escena. Las dependencias a largo plazo significan que el modelo aprende de las primeras observaciones.\n",
    "\n",
    "LSTM intenta atacar la debilidad de RNN. LSTM trabaja con puertas, lo que permiteolvidar datos irrelevantes: estos son:\n",
    "\n",
    "- Forget gates, se creó para clasificar la información necesaria e innecesaria para que LSTM funcione de manera más eficiente que RNN, al hacerlo el valor de la función de activación sigmoide se vuelve cero si la información es irrelevante. Se formula cómo:\n",
    "\n",
    "$$F_t = \\sigma(X_tW_I+h_{t-1}W_f + b_f)$$\n",
    "donde $\\sigma$ es la función de activación, $h_{t−1}$ es el estado oculto anterior, $W_I$ y $W_h$ son pesos y, finalmente, $b_f$ es el parámetro de sesgo en la celda de olvido.\n",
    "\n",
    "- Input gate, se alimenta del paso del tiempo actual, $X_t$, y del estado oculto del paso del tiempo anterior $t-1$. El objetivo del input gate, es determinar el alcance de la información que se debe agregar al estado a largo plazo:\n",
    "$$I_t = \\sigma(\\X_tW_I+h_{t-1}W_h+b_I)$$\n",
    "\n",
    "- Output gate, básicamente determina el alcance de la salida que debe leerse y funciona:\n",
    "$$O_t = \\sigma(X_tW_O+h_{t-1}W_O+b_I)$$\n",
    "\n",
    "Las puertas no son los únicos componentes de LSTM. Los otros componentes son:\n",
    "\n",
    "- Candidate memory All.- Determina en qué medida la información pasa al estado de la celda. De manera diferente, la función de activación en la celda candidata es `tanh` y toma la siguiente forma:\n",
    "$$\\overline{C}_t = \\phi(X_tW_c+h_{t-1}W_c+b_c)$$\n",
    "La celda de memoria permite a LSTM recordar u olvidar la información:\n",
    "$$C_t = F_t\\odot C + t-1+I_t\\odot \\overline{C}_t.$$\n",
    "donde $\\odot$ es el producto hadmard.\n",
    "En esta red recurrente, el estado oculto es una herramienta para hacer circular información. La celda de memoria relaciona la puerta de salida con el estado oculto:\n",
    "$$h_t = \\phi(c_t)\\odot O_t.$$\n",
    "\n",
    "Intentemos predecir el precio de las acciones de Apple y Microsoft utilizando LSTM y veamos cómo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f4d2e9-4041-4eaf-a142-5eb57341002d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_aapl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m     14\u001b[0m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_aapl\u001b[49m, y_aapl,\n\u001b[1;32m     17\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     18\u001b[0m validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.10\u001b[39m)\n\u001b[1;32m     20\u001b[0m start \u001b[38;5;241m=\u001b[39m X_aapl[X_aapl\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m13\u001b[39m]\n\u001b[1;32m     21\u001b[0m x_input \u001b[38;5;241m=\u001b[39m start\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_aapl' is not defined"
     ]
    }
   ],
   "source": [
    "n_steps = 13\n",
    "n_features = 1\n",
    "\n",
    "# Llamar al modelo LSTM e identificar la cantidad de neuronas y características\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, activation='relu',\n",
    "input_shape=(n_steps, n_features),\n",
    "return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256,activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=\n",
    "['mse'])\n",
    "\n",
    "history = model.fit(X_aapl, y_aapl,\n",
    "epochs=400, batch_size=150, verbose=0,\n",
    "validation_split = 0.10)\n",
    "\n",
    "start = X_aapl[X_aapl.shape[0] - 13]\n",
    "x_input = start\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "\n",
    "tempList_aapl = []\n",
    "for i in range(len(diff_test_aapl)):\n",
    "    x_input = x_input.reshape((1, n_steps, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    x_input = np.append(x_input, yhat)\n",
    "    x_input = x_input[1:]\n",
    "    tempList_aapl.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0e40d-c7ce-46ff-8d05-30e831134132",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
